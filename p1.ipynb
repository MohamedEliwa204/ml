{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-24T10:01:26.587568Z",
     "start_time": "2025-12-24T10:01:26.583510Z"
    }
   },
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:01:27.112252Z",
     "start_time": "2025-12-24T10:01:26.633818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"California_Houses.csv\")\n",
    "\n",
    "df.info()"
   ],
   "id": "5a6ded34961b692d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20640 entries, 0 to 20639\n",
      "Data columns (total 14 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Median_House_Value        20640 non-null  float64\n",
      " 1   Median_Income             20640 non-null  float64\n",
      " 2   Median_Age                20640 non-null  int64  \n",
      " 3   Tot_Rooms                 20640 non-null  int64  \n",
      " 4   Tot_Bedrooms              20640 non-null  int64  \n",
      " 5   Population                20640 non-null  int64  \n",
      " 6   Households                20640 non-null  int64  \n",
      " 7   Latitude                  20640 non-null  float64\n",
      " 8   Longitude                 20640 non-null  float64\n",
      " 9   Distance_to_coast         20640 non-null  float64\n",
      " 10  Distance_to_LA            20640 non-null  float64\n",
      " 11  Distance_to_SanDiego      20640 non-null  float64\n",
      " 12  Distance_to_SanJose       20640 non-null  float64\n",
      " 13  Distance_to_SanFrancisco  20640 non-null  float64\n",
      "dtypes: float64(9), int64(5)\n",
      "memory usage: 2.2 MB\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:01:27.154094Z",
     "start_time": "2025-12-24T10:01:27.142582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df.drop(\"Median_House_Value\", axis=1)\n",
    "y = df[\"Median_House_Value\"]"
   ],
   "id": "6ce6af64d21b0889",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:01:27.337157Z",
     "start_time": "2025-12-24T10:01:27.309932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val,y_test = train_test_split(\n",
    "     X_temp, y_temp, test_size=0.50, random_state=42\n",
    ")\n",
    "total_rows = len(df)\n",
    "print(f\"Total rows: {total_rows}\")\n",
    "print(f\"Training set:   {len(X_train)} rows ({len(X_train)/total_rows:.2%})\")\n",
    "print(f\"Validation set: {len(X_val)} rows ({len(X_val)/total_rows:.2%})\")\n",
    "print(f\"Testing set:    {len(X_test)} rows ({len(X_test)/total_rows:.2%})\")"
   ],
   "id": "25c68a8fc28d430c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20640\n",
      "Training set:   14448 rows (70.00%)\n",
      "Validation set: 3096 rows (15.00%)\n",
      "Testing set:    3096 rows (15.00%)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:01:27.460733Z",
     "start_time": "2025-12-24T10:01:27.444684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# scaling the data first so could apply regularization\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "y_train = y_train / 100000\n",
    "y_val = y_val / 100000\n",
    "y_test = y_test / 100000\n"
   ],
   "id": "1fef99123379a0da",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:01:27.484742Z",
     "start_time": "2025-12-24T10:01:27.475839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CustomRegressor:\n",
    "    def __init__(self, X, y, epochs=100, penalty=None,  gradient_method=\"batch\"):\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.penalty=penalty\n",
    "\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.gradient_method = gradient_method\n",
    "    def fit(self,learning_rate=0.01, lambd=0):\n",
    "        self.lambd=lambd\n",
    "        self.learning_rate=learning_rate\n",
    "        n_samples, n_features = self.X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        for i in range(self.epochs):\n",
    "            if self.gradient_method == \"batch\":\n",
    "                y_predicted = np.dot(self.X, self.weights) + self.bias\n",
    "                dw = (1/n_samples) * np.dot(self.X.T, (y_predicted - self.y)) # X.T is the xj in the gradient descent(row reppresent feature and coluumn represent sample) as now each row in X.T represent feature, and as they are vectors the loop is implicity inside them\n",
    "                db = (1/n_samples) * np.sum(y_predicted - self.y)\n",
    "\n",
    "                if self.penalty == \"l2\":\n",
    "                    dw += 2 * self.lambd * (self.weights)\n",
    "                elif self.penalty == \"l1\":\n",
    "                    dw += (self.lambd/ n_samples) * np.sign(self.weights) # diffrentiate the w multiplied with constant so remain the constant with the sign of w\n",
    "\n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias  -= self.learning_rate * db\n",
    "            elif self.gradient_method == \"sgd\":\n",
    "                indices = np.random.permutation(n_samples)\n",
    "                for idx in indices:\n",
    "                    xi = self.X[idx:idx+1]\n",
    "                    yi = self.y.iloc[idx]\n",
    "                    y_pred = np.dot(xi, self.weights) + self.bias\n",
    "                    dw = np.dot(xi.T, (y_pred - yi)).flatten()\n",
    "                    db = (y_pred - yi).item()\n",
    "\n",
    "                    if self.penalty == \"l2\":\n",
    "                        dw +=  (2*self.lambd/ n_samples) * (self.weights)\n",
    "                    elif self.penalty == \"l1\":\n",
    "                        dw += (self.lambd/n_samples) * np.sign(self.weights)\n",
    "\n",
    "                    self.weights -= self.learning_rate * dw\n",
    "                    self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias"
   ],
   "id": "26144d4b7ac582a5",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:01:27.497748Z",
     "start_time": "2025-12-24T10:01:27.494073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "learning_rates = [0.001, 0.0001, 0.00001, 0.000001]\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1, 10, 100]"
   ],
   "id": "84e03db322e056e6",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:01:27.511985Z",
     "start_time": "2025-12-24T10:01:27.506088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "def grid_search(X_train, y_train, X_val, y_val, penalty_type, optimizer_type):\n",
    "    print(f\"--- Tuning {penalty_type} ({optimizer_type}) ---\")\n",
    "\n",
    "    best_mse = float('inf')\n",
    "    best_params = {}\n",
    "\n",
    "    model = CustomRegressor(X_train, y_train, 1, penalty_type, optimizer_type)\n",
    "\n",
    "    for lr, l in itertools.product(learning_rates, lambdas):\n",
    "        model.fit(lr, l)\n",
    "        preds = model.predict(X_val)\n",
    "        mse = mean_squared_error(y_val, preds)\n",
    "\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_params = {'learning_rate': lr, 'lambd': l}\n",
    "\n",
    "    if penalty_type == None:\n",
    "        best_params['lambd'] = 0\n",
    "\n",
    "    print(f\"Best Params: {best_params}\")\n",
    "    print(f\"Best Validation MSE: {best_mse:.2f}\")\n",
    "    print(\"-\" * 30)\n",
    "    return best_params\n"
   ],
   "id": "7808530ebedd9e94",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:01:49.475598Z",
     "start_time": "2025-12-24T10:01:27.521826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_ridge_batch = grid_search(X_train_scaled, y_train, X_val_scaled, y_val, \"l2\", \"batch\")\n",
    "best_lasso_batch = grid_search(X_train_scaled, y_train, X_val_scaled, y_val, \"l1\", \"batch\")\n",
    "\n",
    "best_ridge_sgd = grid_search(X_train_scaled, y_train, X_val_scaled, y_val, \"l2\", \"sgd\")\n",
    "\n",
    "best_linear_batch = grid_search(X_train_scaled, y_train, X_val_scaled, y_val, None, \"batch\")\n",
    "\n",
    "best_linear_sgd = grid_search(X_train_scaled, y_train, X_val_scaled, y_val, None, \"sgd\")\n",
    "\n",
    "\n",
    "\n",
    "best_lasso_sgd = grid_search(X_train_scaled, y_train, X_val_scaled, y_val, \"l1\", \"sgd\")\n",
    "\n"
   ],
   "id": "69df8a26ec071dc3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuning l2 (batch) ---\n",
      "Best Params: {'learning_rate': 0.001, 'lambd': 0}\n",
      "Best Validation MSE: 5.57\n",
      "------------------------------\n",
      "--- Tuning l1 (batch) ---\n",
      "Best Params: {'learning_rate': 0.001, 'lambd': 0}\n",
      "Best Validation MSE: 5.57\n",
      "------------------------------\n",
      "--- Tuning l2 (sgd) ---\n",
      "Best Params: {'learning_rate': 0.001, 'lambd': 10}\n",
      "Best Validation MSE: 0.51\n",
      "------------------------------\n",
      "--- Tuning None (batch) ---\n",
      "Best Params: {'learning_rate': 0.001, 'lambd': 0}\n",
      "Best Validation MSE: 5.57\n",
      "------------------------------\n",
      "--- Tuning None (sgd) ---\n",
      "Best Params: {'learning_rate': 0.001, 'lambd': 0}\n",
      "Best Validation MSE: 0.50\n",
      "------------------------------\n",
      "--- Tuning l1 (sgd) ---\n",
      "Best Params: {'learning_rate': 0.001, 'lambd': 0}\n",
      "Best Validation MSE: 0.51\n",
      "------------------------------\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:03:05.523675Z",
     "start_time": "2025-12-24T10:01:49.593263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "final_configs = [\n",
    "    (\"Linear Regression (Batch)\", best_linear_batch, None, \"batch\"),\n",
    "    (\"Linear Regression (SGD)\",   best_linear_sgd,   None, \"sgd\"),\n",
    "    (\"Ridge Regression (Batch)\",  best_ridge_batch,  \"l2\", \"batch\"),\n",
    "    (\"Ridge Regression (SGD)\",    best_ridge_sgd,    \"l2\", \"sgd\"),\n",
    "    (\"Lasso Regression (Batch)\",  best_lasso_batch,  \"l1\", \"batch\"),\n",
    "    (\"Lasso Regression (SGD)\",    best_lasso_sgd,    \"l1\", \"sgd\"),\n",
    "]\n",
    "\n",
    "results_data = []\n",
    "\n",
    "print(\"--- FINAL TESTING RESULTS ---\\n\")\n",
    "\n",
    "# 2. Loop through each configuration\n",
    "for name, params, penalty, optimizer in final_configs:\n",
    "\n",
    "\n",
    "    model = CustomRegressor(\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        epochs=100,\n",
    "        penalty=penalty,\n",
    "        gradient_method=optimizer\n",
    "    )\n",
    "\n",
    "\n",
    "    model.fit(params['learning_rate'],params['lambd'])\n",
    "\n",
    "\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    results_data.append({\n",
    "        \"Model\": name,\n",
    "        \"Optimizer\": optimizer.upper(),\n",
    "        \"Best Lambda\": params['lambd'],\n",
    "        \"Best LR\": params['learning_rate'],\n",
    "        \"Test MSE\": mse,\n",
    "        \"Test R2\": r2\n",
    "    })\n",
    "\n",
    "    print(f\"{name}: MSE={mse:.2f}, R2={r2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\n--- LEADERBOARD ---\")\n",
    "display(results_df.sort_values(by=\"Test MSE\", ascending=True))"
   ],
   "id": "fa52f994d611260a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL TESTING RESULTS ---\n",
      "\n",
      "Linear Regression (Batch): MSE=4.63, R2=-2.4999\n",
      "Linear Regression (SGD): MSE=0.44, R2=0.6664\n",
      "Ridge Regression (Batch): MSE=4.63, R2=-2.4999\n",
      "Ridge Regression (SGD): MSE=0.44, R2=0.6646\n",
      "Lasso Regression (Batch): MSE=4.63, R2=-2.4999\n",
      "Lasso Regression (SGD): MSE=0.45, R2=0.6614\n",
      "\n",
      "--- LEADERBOARD ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                       Model Optimizer  Best Lambda  Best LR  Test MSE  \\\n",
       "1    Linear Regression (SGD)       SGD            0    0.001  0.441074   \n",
       "3     Ridge Regression (SGD)       SGD           10    0.001  0.443509   \n",
       "5     Lasso Regression (SGD)       SGD            0    0.001  0.447688   \n",
       "0  Linear Regression (Batch)     BATCH            0    0.001  4.627913   \n",
       "2   Ridge Regression (Batch)     BATCH            0    0.001  4.627913   \n",
       "4   Lasso Regression (Batch)     BATCH            0    0.001  4.627913   \n",
       "\n",
       "    Test R2  \n",
       "1  0.666437  \n",
       "3  0.664595  \n",
       "5  0.661435  \n",
       "0 -2.499869  \n",
       "2 -2.499869  \n",
       "4 -2.499869  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Optimizer</th>\n",
       "      <th>Best Lambda</th>\n",
       "      <th>Best LR</th>\n",
       "      <th>Test MSE</th>\n",
       "      <th>Test R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear Regression (SGD)</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.441074</td>\n",
       "      <td>0.666437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge Regression (SGD)</td>\n",
       "      <td>SGD</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.443509</td>\n",
       "      <td>0.664595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lasso Regression (SGD)</td>\n",
       "      <td>SGD</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.447688</td>\n",
       "      <td>0.661435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression (Batch)</td>\n",
       "      <td>BATCH</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.627913</td>\n",
       "      <td>-2.499869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge Regression (Batch)</td>\n",
       "      <td>BATCH</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.627913</td>\n",
       "      <td>-2.499869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso Regression (Batch)</td>\n",
       "      <td>BATCH</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4.627913</td>\n",
       "      <td>-2.499869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T10:03:06.132327Z",
     "start_time": "2025-12-24T10:03:06.129578Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3d063fff8a757bc1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
